---
title: "DSA 2021 - TD R"
author: "Fabien FAIVRE"
date: "18 juillet 2021"
output: 
  html_document:
    toc: true
    toc_float: true
    toc_collapsed: true
    toc_depth: 4
    number_sections: true
    theme: lumen
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE)
```

# Présentation du projet

Ce projet s'inscrit dans le cadre de la formation [DSA](https://www.institut-du-risk-management.fr/nos-formations/formations-certifiantes/certificat-data-science-pour-actuariat/), et évalue l'apprentissage de R réalisé auprès de Robin RYDER.

Le thème de ce projet est libre.

---
> J'ai choisi d'étudier une série d'articles qui m'ont frappé sur la comparaison des approches classiques par modèles linéaires généralisés `GLM`) et par machine learning (ci après `ML`) dans le cadre de la tarification en assurance non-vie et la manière dont elles pouvaient se compléter.

---

Dans le suite de ce rapport, on ne s'intéressera qu'à la modélisation de la fréquence.

Soit $N_{i}$ le nombre de sinistre de la police ${i}$, alors dans le modèle Poisson homogène on suppose $N_{i} \sim \mathscr{P}(\lambda \nu_{i})$ où $\nu_{i}$ est l'exposition au risque de la police $i$, $\lambda$ est la fréquence de sinistre annuelle (homogène sur le portefeuille)et $\mathscr{P}$ la loi de Poisson.
On pose $Y_{i} = \frac{N_{i}}{\nu_{i}}$ la fréquence annuelle des sinistres pour la police $i$ et $\mathbf{x_{i}} = (x_{1},x_{2}, \cdots, x_{q}) $ les $q$ caractéristiques de la police $i$.

En tarification, on suppose dans les faits que $\lambda$ dépende de $\mathbf{x}$, de sorte que $\mathbb{P}[Y=k/\nu] = \mathbb{P}[N=k] = e^{-\lambda(\mathbf{x})\nu} \frac{(\lambda(\mathbf{x})\nu)^{k}}{k!}$ avec $ln(\lambda(\mathbf{x})) = \beta_{0} + \beta_{1}x_{1} + \beta_{2}x_{2} + \cdots + \beta_{q}x_{q} \overset{def}{=} \langle \mathbf{\beta}, \mathbf{x} \rangle$ en complétant $\mathbf{x}$ par $x_{0} = 1$.

Lorsqu'on utilise les approches classiques de GLM, on estime $\hat{\lambda}$ en utilisant l'estimateur du maximum de vraisemblance $\hat{\beta}$ de $\beta$.

Cet estimateur peut être obtenu soit en maximisant la log vraisemblance, soit de manière équivalente en minimisant la déviance de Poisson.

Une première adaptation des modèles classiques consiste donc à **définir la déviance de poisson comme fonction de perte des algorithmes de ML**.

C'est notamment l'approche préconisée par dans [Data Analytics for Non-Life Insurance Pricing](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=2870308) de Mario V. Wuthrich et Christoph Buser

En complément, lorsqu'on utilise un GLM incluant un intercept et sa fonction de lien canonique, l'utilisatiuon d'un estimateur du maximum de vraisemblance apporte de bonnes propriétés pour son usage en tarification dont celle dite de **propriété d'équilibre** (Property 2.4 p33 de [Data Analytics for Non-Life Insurance Pricing](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=2870308)), qui précise que :

$\displaystyle\sum_{i=1}^{n} \nu_i\hat{\lambda}(\mathbf{x_{i}}) = \displaystyle\sum_{i=1}^{n} \nu_i exp\langle \mathbf{\hat{\beta}}, \mathbf{x_{i}} \rangle = \displaystyle\sum_{i=1}^{n} N_{i}$

De sorte que le nombre total de sinistres estimés est égale au nombre total de sinistres observés. 

Cette propriété est même plus étendue dans certains modèles GLM, notamment lorsque le modèle poissonien avec lien log (exemple (b) p423 de [A systematic relationship between minimum bias and gener- alized linear models](https://www.casact.org/sites/default/files/2021-02/pubs_proceed_proceed99_99317.pdf)) :  dans ce cas l'équilibre existe au globaldu portefeuille, mais aussi pour chaque classe des variables catégorielles incluses dans le modèle.

**Ces propriétés d'équilibres sont critiques pour un assureur** dans un exercice de tarification, puisqu'elles assurent qu'une structure tarifaire basée sur cette approche lui permettra d'équilibrer (au moins théoriquement) ses encaissement avec les sinistres qu'il s'est engagé à indemniser.
En outre, ces approches sont très intuitives et ont en partie contribué au succès des GLM en assurance.

```
Or il semble que l'utilisation directe de modèles de ML, mêmes adaptés pour minimiser la déviance, ne présentent pas ces garanties.
```

En effet, si les modèles de ML permettent une meilleurs corrélation avec la réponse individuelle de chaque police, aucune garantie n'est expressément introduite au global du portefeuille ce qui peut produire des divergences, potentiellement importantes, entre la somme des primes demandées et les sinistres que l'assureur s'est engagé à régler.

Cet effet est attribué par M Wütrich aux conditions d'arrêt précoce des algorithmes d'optimisation utilisés par les modèles ML.

L'article [Autocalibration and Tweedie-dominance for Insurance Pricing with Machine Learning](https://arxiv.org/abs/2103.03635) de Michel Denuit, Arthur Charpentier et Julien Trufin repart de ce constat et 







1. [Data Analytics for Non-Life Insurance Pricing](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=2870308) de Mario V. Wuthrich et Christoph Buser
   Cet article détaille les mathématiques derrières l'utilisation des modèles GLM et des principaux modèles ML, y compris les réseaux de neurones de type feed forward

2. [Peeking into the black box](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3595944) de Christian Lorentzen et Michael Mayer
   Cet article met en avant une utilisation des techniques d'explicabilité des modèles ML pour comprendre la manière dont ces modèles combinent les informations en entrée pour arriver à leurs prédictions. Ce faisant le praticien est invité à :
    - identifier le gain marginal du modèle ML par rapport au modèle GLM généralement en place et en cas d'écart significatif à
    - identifier les éléments appris par le modèles ML (effets non linéaires adns les variables simples, principales intéractions...) qui peuvent être réintégrés dans le modè-le GLM afin d'en réduire l'écart en termes de performance

3. [Autocalibration and Tweedie-dominance for Insurance Pricing with Machine Learning](https://arxiv.org/abs/2103.03635) de Michel Denuit, Arthur Charpentier et Julien Trufin
   Cet article est le plus intriguant des 3












nous avons eu l'occasion d'analyser un jeu de données de tarification MRH à l'occasion d'un Hackathon.

Du fait des contraintes propres au Hackathon je n'avais pas eu l'occasion de finaliser l'analyse de ce jeu de données. Par ailleurs, pour le hackathon, le choix s'était porté sur le logiciel Python.

Ce TD est donc l'occasion de poursuivre l'analyse de ce jeu de données. Toutefois, cet exercice est aussi l'occasion d'explorer les apports de deux articles :

1. [Peeking into the black box](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3595944) de Christian Lorentzen et Michael Mayer

2. [Autocalibration and Tweedie-dominance for Insurance Pricing with Machine Learning](https://arxiv.org/abs/2103.03635) de Michel Denuit, Arthur Charpentier et Julien Trufin

Ces articles s'appuient sur les travaux de M [Wütrich](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=2870308) qui proposaient d'adapter la fonction de perte des modèles de machine learning au cas poissonien (déviance de poisson) pour pouvoir appliquer les techniques de ML à des problèmes d'estimation de la fréquence de sinistres.

Cette approche diffère des consignes du Hackathon qui préconisaient de minimiser le RMSE. En effet, cette fonction d'erreur suppose une distribution normale, ce qui dans les cas de tarfication où la fréquence de sinistre est faible n'est pas réaliste.

Dans le suite de ce rapport, on ne s'intéressera qu'à la modélisation de la fréquence.

Soit $N_{i}$ le nombre de sinistre de la police ${i}$, alors dans le modèle Poisson homogène on suppose $N_{i} \sim \mathscr{P}(\lambda \nu_{i})$ où $\nu_{i}$ est l'exposition au risque de la police $i$, $\lambda$ est la fréquence de sinistre annuelle (homogène sur le portefeuille)et $\mathscr{P}$ la loi de Poisson.
On pose $Y_{i} = \frac{N_{i}}{\nu_{i}}$ la fréquence annuelle des sinistres pour la police $i$ et $\mathbf{x_{i}} = (x_{1},x_{2}, \cdots, x_{q}) $ les $q$ caractéristiques de la police $i$.

En tarification, on suppose dans les faits que $\lambda$ dépende de $\mathbf{x}$, de sorte que $\mathbb{P}[Y=k/\nu] = \mathbb{P}[N=k] = e^{-\lambda(\mathbf{x})\nu} \frac{(\lambda(\mathbf{x})\nu)^{k}}{k!}$ avec $ln(\lambda(\mathbf{x})) = \beta_{0} + \beta_{1}x_{1} + \beta_{2}x_{2} + \cdots + \beta_{q}x_{q} \overset{def}{=} \langle \mathbf{\beta}, \mathbf{x} \rangle$

Lorsqu'on utilise les approches classiques de GLM, on estime $\hat{\lambda}$ en utilisant l'estimateur du maximum de vraisemblance $\hat{\beta}$ de $\beta$.

Cet estimateur peut être obtenu soit en maximisant la log vraisemblance, soit en minimisant la déviance de Poisson.

Une première adaptation des modèles classiques consiste donc à définir la déviance de poisson comme fonction de perte des algorithmes de ML.

En complément, lorsqu'on utilise un GLM incluant un intercept et sa fonction de lien canonique, l'utilisatiuon d'un estimateur du maximum de vraisemblance apporte de bonnes propriétés pour son usage en tarification dont celle dite d'*équilibre* (Property 2.4 p33 de [Data Analytics for Non-Life Insurance Pricing](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=2870308)), qui précise que :

$\displaystyle\sum_{i=1}^{n} \nu_i\hat{\lambda}(\mathbf{x_{i}}) = \displaystyle\sum_{i=1}^{n} \nu_i exp\langle \mathbf{\hat{\beta}}, \mathbf{x_{i}} \rangle = \displaystyle\sum_{i=1}^{n} N_{i}$

De sorte que le nombre total de sinistres estimés est égale au nombre total de sinistres observés. 

Cette propriété semble même plus étendue dans les modèles GLM puisque cet équilibre se retrouve au global du portefeuille et pour toute modalité des variables catégorielles (dixit l'introduction de [Autocalibration and Tweedie-dominance for Insurance Pricing with Machine Learning](https://arxiv.org/abs/2103.03635))

Il semble toutefois que ce ne soit pas le cas pour les modèles de ML, même lorque ceux-ci sont paramétrer pour minimiser la déviance de Poisson. [Wütrich](https://www.tandfonline.com/doi/pdf/10.1080/24754269.2021.1877960?needAccess=true) attribue cet effet aux méthodes d'arrêt précoce des algorithme d'optimisation utilisés.
L'explication qualitative produite est que les modèles de ML, par leur plus grande souplesse, permettent un meilleurajustement aux réponses individuelles. Toutefois, ceux-ci n'incluent pas de contraintes d'équilibre au niveau du portefeuille, de sorte que ce meilleur ajustement local peut créer un biais (potentiellement important) au niveau global.

Si cette propriété n'est pas respectée, il s'agit là d'un frein important à l'utilisation des modèles ML (non modifiés) en tarification, puisque l'assureur ne disposerait plus de garanties d'équilibre financier global entre son encaissement et les sinsitres qu'il s'est engagé à régler.

C'est le sens de l'article [Autocalibration and Tweedie-dominance for Insurance Pricing with Machine Learning](https://arxiv.org/abs/2103.03635)) qui propose de se servir des primes estimées lors d'ue première étape par les modèles ML comme d'entrants pour une deuxième étape d'autocalibration locale. Cette méthode consiste à ajuster des modèles GLM locaux réduits à un intercept sur des ensembles de points qui sont définis à partir de la proximité des primes estimées par ML (l'étape de ML rassemble les observations qui se ressemblent, l'étape locale GLM lisse les écarts entre observations voisines pour retrouver les propriétés d'équilibre). 



Soit $\mu_{i}(x_{i}) = \mathbb{E}[Y_{i}|\mathbf{X} = \mathbf{x_{i}}]$
Dans la 






Ce TD est donc l'occasion de poursuivre l'analyse de ce jeu de données et d'explorer l'usage de packages et d'approches développés par MV Wütrich et disponible sur le site de l'[association suisse des actuaires](https://www.actuarialdatascience.org/ADS-Lectures/Courses/).

# Analyse

## Mise en place de l'environnement

Le code suivant est un ensemble d'utilitaire pour naviguer dans les répertoires relatifs du projet et installer ou charger les packages nécessaires à l'analyse :

```{r echo=T, results='hide', message=FALSE, warning=FALSE}
if (!require("here")){ 
  install.packages("here") 
  library("here")
}

set_here

LoadPackage <- function (load.lib=c("")) {

  install.lib<-load.lib[!load.lib %in% installed.packages()]
  for(lib in install.lib) install.packages(lib,dependencies=TRUE)
  sapply(load.lib,require,character=TRUE)
  
}


LoadPackage(c('ggplot2', 'dplyr', 'formattable', 'DT', 'tidyr', 'caret', 'plotly', 'xgboost', 'flashlight', 'MetricsWeighted', 'corrplot', 'lsr', 'h2o'))
```

## code

```{r echo=FALSE}

plot_obs<-function(dt,feature) {
  
      fig <- dt %>% plot_ly()
      fig <- fig %>% add_trace(
                     x= dt[[feature]], y= dt[['EXPO']], type = 'bar', name = 'Exposition',
                     marker = list(color = '#C9EFF9'),
                     hoverinfo = "text",
                     text = ~paste(round(EXPO,0), ' années risque')
                  )
      fig<- fig %>% add_trace(
                     x= dt[[feature]], y= dt[['Freq']], type = 'scatter', mode = 'lines', yaxis = 'y2', name= 'Fréquence',
                     hoverinfo = "text",
                     text = ~paste(round(Freq*100,2), '%')
                    )
      fig <- fig %>% layout(title = paste0('Fréquence de sinistres par ', feature),
               xaxis = list(title = ""),
               yaxis = list(side = 'left', title = 'Exposition', showgrid = FALSE, zeroline = FALSE),
               yaxis2 = list(side = 'right', overlaying = "y", title = 'Fréquence', showgrid = FALSE, zeroline = FALSE))
      
      
      return(fig)
}

```

```{r echo=FALSE}


plot_obs_2<-function(df,feature,use_year=T) {

    if (use_year) {
          dt <-df %>%
          group_by(df[[feature]], ANNEE, .drop=FALSE) %>%
          summarise(Freq = sum(NB*EXPO)/sum(EXPO), nb_lignes = n(), EXPO=sum(EXPO) ,  pct_EXPO = sum(EXPO) /  sum(df$EXPO)) %>%
          rename(feat = 'df[[feature]]') 
    } else {
          dt <-df %>%
          group_by(df[[feature]], .drop=FALSE) %>%
          summarise(Freq = sum(NB*EXPO)/sum(EXPO), nb_lignes = n(), EXPO=sum(EXPO) ,  pct_EXPO = sum(EXPO) /  sum(df$EXPO)) %>%
          rename(feat = 'df[[feature]]') 
    }
    
    
    
    fig <-plot_ly()
    fig <- fig %>% layout(title = paste0('Fréquence de sinistres par ', feature),
             xaxis = list(title = feature),
             yaxis = list(side = 'left', title = 'Exposition', showgrid = FALSE, zeroline = FALSE),
             yaxis2 = list(side = 'right', overlaying = "y", title = 'Fréquence', showgrid = FALSE, zeroline = FALSE))
    
    if (use_year) {
        for (year in unique(dt$ANNEE)) {
        
              dt_temp <- dt %>% filter(ANNEE==year)
              fig <- fig %>% add_trace( 
                             x= dt_temp$feat, y= dt_temp$EXPO, type = 'bar', name = paste0('Exposition_', year),
                             hovertemplate = 'Expo: %{y:.0f}<br>'
                          )
              fig<- fig %>% add_trace(
                             x= dt_temp$feat, y= dt_temp$Freq, type = 'scatter', mode = 'lines', yaxis = 'y2', name= paste0('Fréquence_', year),
                             hovertemplate = 'Freq: %{y:.2%}<br>'
                            )
        
        }
    } else {
              dt_temp <- dt 
              fig <- fig %>% add_trace( 
                             x= dt_temp$feat, y= dt_temp$EXPO, type = 'bar', name = 'Exposition',
                             hovertemplate = 'Expo: %{y:.0f}<br>'
                          )
              fig<- fig %>% add_trace(
                             x= dt_temp$feat, y= dt_temp$Freq, type = 'scatter', mode = 'lines', yaxis = 'y2', name= 'Fréquence',
                             hovertemplate = 'Freq: %{y:.2%}<br>'
                            )
      
    }
    
    return(fig) 
}


```

```{r echo=FALSE}
resume <- function(df, feature) {

    dt <-df %>%
      group_by(df[[feature]], .drop=FALSE) %>%
      summarise(Freq = sum(NB*EXPO)/sum(EXPO), nb_lignes = n(), EXPO=sum(EXPO) ,  pct_EXPO = sum(EXPO) /  sum(df$EXPO ))
    
    colnames(dt)[1] <- feature
    
    
    table<- formattable(dt,
                align = c("l", rep("r", NCOL(dt) - 1)),
                list(
                    Freq = percent,
                    nb_lignes= accounting,
                    EXPO = accounting,
                    pct_EXPO = percent
                )
    )
  
    fig1<- plot_obs_2(df, feature, use_year = F)
    
    fig2<- plot_obs_2(df, feature, use_year = T)
    
    return(list(dt=dt, table=table, fig=fig1, figyr=fig2))
}
```

# Présentation du jeu de données

Le jeu de données est constitué de 4 fichiers :

```{r data}
list.files(path=here('data', 'raw'))
```

Le projet suit un portefeuille d'assurance MRH suivi sur plusieurs années de 2016 à 2018 inclus. L'objectif initial du Hackathon est d'estimer les primes 2019 sur un échantillon de contrats.

## expo_train

Le fichier `expo_train.csv` contient la liste des contrats en risques historiquement suivis :

```{r}
expo_train = read.table(file = here('data', 'raw', 'expo_train.csv'), header=T, sep=',', dec='.', encoding = 'UTF-8', stringsAsFactors = F)

datatable(head(expo_train))

str(expo_train)
```

Les données ont été prétraitées pour disposer d'**une ligne par période de risque annuelle homogène**.

Pour des raisons d'anonymisation, les identifiants contrats ont été supprimés de sorte qu'il n'est pas possible de suivre l'évolution du risque d'une période sur l'autre. Pour la même raison, il ne sera pas possible dans la construction des jeux de validation et de test de s'assurer que toute l'expérience d'un même assuré est bien intégralement dans un jeu d'entraînement, de validation ou de test.

La signification des colonnes présentes est la suivante :

### **`X`**:

variable non nommée. Il s'agit d'un artefact du processus de création du fichier initial. **On ne la prend pas en compte**

### **`EXPO`** :

exposition en année risque du contrat. Sa valeur précalculée pour chaque ligne est comprise entre 0 et 1

```{r echo=FALSE}
formattable(summary(expo_train$EXPO))

expo_train %>%
  ggplot( aes(x=EXPO)) +
    geom_density(fill="#69b3a2", color="#e9ecef", alpha=0.8) +
    ggtitle("Distribution de EXPO")
  
```

### **`FORMULE`** :

variable catégorielle codant la formule du contrat comprenant 3 niveaux `MEDIUM`, `ESSENTIEL` et `CONFORT`

```{r echo=FALSE}
expo_tot <- sum(expo_train$EXPO)


dt <-expo_train %>%
  group_by(FORMULE) %>%
  summarise(nb_lignes = n(), EXPO=sum(EXPO) ,  pct_EXPO = sum(EXPO) /  expo_tot )


formattable(dt,
            align=c('l','r','r', 'r'),
            list(
                nb_lignes= accounting,
                EXPO = accounting,
                pct_EXPO = percent
            )
)


dt %>%
  ggplot( aes(x=as.factor(FORMULE), y=EXPO, fill=as.factor(FORMULE))) +
    geom_bar(stat='identity') +
    ggtitle('Exposition par FORMULE')+
    xlab('FORMULE') +
    ylab('exposition') +
    labs(fill = 'FORMULE')
  
```

### **`TYPE_RESIDENCE`** :

variable catégorielle codant le fait que le bien est une résidence `PRINCIPALE`, ou `SECONDAIRE`

```{r echo=FALSE}

expo_tot <- sum(expo_train$EXPO)


dt <-expo_train %>%
  group_by(TYPE_RESIDENCE) %>%
  summarise(nb_lignes = n(), EXPO=sum(EXPO) ,  pct_EXPO = sum(EXPO) /  expo_tot )


formattable(dt,
            align=c('l','r','r', 'r'),
            list(
                nb_lignes= accounting,
                EXPO = accounting,
                pct_EXPO = percent
            )
)

dt %>%
  ggplot( aes(x=as.factor(TYPE_RESIDENCE), y=EXPO, fill=as.factor(TYPE_RESIDENCE))) +
    geom_bar(stat='identity') +
    ggtitle('Exposition par TYPE_RESIDENCE')+
    xlab('TYPE_RESIDENCE') +
    ylab('exposition') +
    labs(fill = 'TYPE_RESIDENCE')
  
```

### **`TYPE_HABITATION`** :

variable catégorielle codant le fait que le bien est un `APPARTEMENT`, ou une `MAISON`

```{r echo=FALSE}

expo_tot <- sum(expo_train$EXPO)


dt <-expo_train %>%
  group_by(TYPE_HABITATION) %>%
  summarise(nb_lignes = n(), EXPO=sum(EXPO) ,  pct_EXPO = sum(EXPO) /  expo_tot )


formattable(dt,
            align=c('l','r','r', 'r'),
            list(
                nb_lignes= accounting,
                EXPO = accounting,
                pct_EXPO = percent
            )
)

dt %>%
  ggplot( aes(x=as.factor(TYPE_HABITATION), y=EXPO, fill=as.factor(TYPE_HABITATION))) +
    geom_bar(stat='identity') +
    ggtitle('Exposition par TYPE_HABITATION')+
    xlab('TYPE_HABITATION') +
    ylab('exposition') +
    labs(fill = 'TYPE_HABITATION')
  
```

### **`NB_PIECES`** :

variable numérique indiquant le nombre de pièces du logement

```{r echo=FALSE}

expo_tot <- sum(expo_train$EXPO)


dt <-expo_train %>%
  group_by(NB_PIECES) %>%
  summarise(nb_lignes = n(), EXPO=sum(EXPO) ,  pct_EXPO = sum(EXPO) /  expo_tot )


formattable(dt,
            align=c('l','r','r', 'r'),
            list(
                nb_lignes= accounting,
                EXPO = accounting,
                pct_EXPO = percent
            )
)

dt %>%
  ggplot( aes(x=as.factor(NB_PIECES), y=EXPO, fill=as.factor(NB_PIECES))) +
    geom_bar(stat='identity') +
    ggtitle('Exposition par NB_PIECES')+
    xlab('NB_PIECES') +
    ylab('exposition') +
    labs(fill = 'NB_PIECES')
  
```

### **`SITUATION_JURIDIQUE`** :

variable catégorielle indiquant si le souscripteur est prorpiétaire (`PROPRIO`) ou locataire (`LOCATAIRE`) du logement assuré

```{r echo=FALSE}
expo_tot <- sum(expo_train$EXPO)


dt <-expo_train %>%
  group_by(SITUATION_JURIDIQUE) %>%
  summarise(nb_lignes = n(), EXPO=sum(EXPO) ,  pct_EXPO = sum(EXPO) /  expo_tot )


formattable(dt,
            align=c('l','r','r', 'r'),
            list(
                nb_lignes= accounting,
                EXPO = accounting,
                pct_EXPO = percent
            )
)


dt %>%
  ggplot( aes(x=as.factor(SITUATION_JURIDIQUE), y=EXPO, fill=as.factor(SITUATION_JURIDIQUE))) +
    geom_bar(stat='identity') +
    ggtitle('Exposition par SITUATION_JURIDIQUE')+
    xlab('SITUATION_JURIDIQUE') +
    ylab('exposition') +
    labs(fill = 'SITUATION_JURIDIQUE')
  
```

### **`NIVEAU_JURIDIQUE`** :

variable catégorielle codant le niveau de couverture de la garantie juridique

```{r echo=FALSE}
expo_tot <- sum(expo_train$EXPO)


dt <-expo_train %>%
  group_by(NIVEAU_JURIDIQUE) %>%
  summarise(nb_lignes = n(), EXPO=sum(EXPO) ,  pct_EXPO = sum(EXPO) /  expo_tot )


formattable(dt,
            align=c('l','r','r', 'r'),
            list(
                nb_lignes= accounting,
                EXPO = accounting,
                pct_EXPO = percent
            )
)


dt %>%
  ggplot( aes(x=as.factor(NIVEAU_JURIDIQUE), y=EXPO, fill=as.factor(NIVEAU_JURIDIQUE))) +
    geom_bar(stat='identity') +
    ggtitle('Exposition par NIVEAU_JURIDIQUE')+
    xlab('NIVEAU_JURIDIQUE') +
    ylab('exposition') +
    labs(fill = 'NIVEAU_JURIDIQUE')
  
```

### **`VALEUR_DES_BIENS`** :

variable numérique reflétant la valeur couverte du contenu du logement

```{r echo=FALSE}
expo_tot <- sum(expo_train$EXPO)


dt <-expo_train %>%
  group_by(VALEUR_DES_BIENS) %>%
  summarise(nb_lignes = n(), EXPO=sum(EXPO) ,  pct_EXPO = sum(EXPO) /  expo_tot )


formattable(dt,
            align=c('l','r','r', 'r'),
            list(
                nb_lignes= accounting,
                EXPO = accounting,
                pct_EXPO = percent
            )
)


dt %>%
  ggplot( aes(x=as.factor(VALEUR_DES_BIENS), y=EXPO, fill=as.factor(VALEUR_DES_BIENS))) +
    geom_bar(stat='identity') +
    ggtitle('Exposition par VALEUR_DES_BIENS')+
    xlab('VALEUR_DES_BIENS') +
    ylab('exposition') +
    labs(fill = 'VALEUR_DES_BIENS')
  
```

### **`OBJETS_DE_VALEUR`** :

variable catégorielle codant le niveau de couverture des objets

```{r echo=FALSE}
expo_tot <- sum(expo_train$EXPO)


dt <-expo_train %>%
  group_by(OBJETS_DE_VALEUR) %>%
  summarise(nb_lignes = n(), EXPO=sum(EXPO) ,  pct_EXPO = sum(EXPO) /  expo_tot )


formattable(dt,
            align=c('l','r','r', 'r'),
            list(
                nb_lignes= accounting,
                EXPO = accounting,
                pct_EXPO = percent
            )
)


dt %>%
  ggplot( aes(x=as.factor(OBJETS_DE_VALEUR), y=EXPO, fill=as.factor(OBJETS_DE_VALEUR))) +
    geom_bar(stat='identity') +
    ggtitle('Exposition par OBJETS_DE_VALEUR')+
    xlab('OBJETS_DE_VALEUR') +
    ylab('exposition') +
    labs(fill = 'OBJETS_DE_VALEUR')
  
```

### **`ZONIER`** :

variable catégorielle codant la zone du bien assuré. Le code est constitué d'une lettre représentant une zone et d'un nombre représentant une partie de la zone

```{r echo=FALSE}
expo_tot <- sum(expo_train$EXPO)


dt <-expo_train %>%
  group_by(ZONIER) %>%
  summarise(nb_lignes = n(), EXPO=sum(EXPO) ,  pct_EXPO = sum(EXPO) /  expo_tot )

region <- gsub('[0-9]', '', dt$ZONIER)

formattable(dt,
            align=c('l','r','r', 'r'),
            list(
                nb_lignes= accounting,
                EXPO = accounting,
                pct_EXPO = percent
            )
)


dt %>%
  ggplot( aes(x=as.factor(ZONIER), y=EXPO, fill=as.factor(region))) +
    geom_bar(stat='identity') +
    ggtitle('Exposition par ZONIER')+
    xlab('ZONIER') +
    ylab('exposition') +
    labs(fill = 'ZONIER') +
    theme(axis.text.x = element_text(angle = 45, hjust = 1))
  
```

### **`NBSIN_TYPE1_AN1`** :

variable numérique indiquant le nombre de sinistre de type 1 l'année précédente

```{r echo=FALSE}
expo_tot <- sum(expo_train$EXPO)


dt <-expo_train %>%
  group_by(NBSIN_TYPE1_AN1) %>%
  summarise(nb_lignes = n(), EXPO=sum(EXPO) ,  pct_EXPO = sum(EXPO) /  expo_tot )


formattable(dt,
            align=c('l','r','r', 'r'),
            list(
                nb_lignes= accounting,
                EXPO = accounting,
                pct_EXPO = percent
            )
)


dt %>%
  ggplot( aes(x=as.factor(NBSIN_TYPE1_AN1), y=EXPO, fill=as.factor(NBSIN_TYPE1_AN1))) +
    geom_bar(stat='identity') +
    ggtitle('Exposition par NBSIN_TYPE1_AN1')+
    xlab('NBSIN_TYPE1_AN1') +
    ylab('exposition') +
    labs(fill = 'NBSIN_TYPE1_AN1')
  
```

### **`NBSIN_TYPE1_AN2`** :

variable numérique indiquant le nombre de sinistre de type 1 il y a 2 ans

**ATTENTION** : comme la table ci-dessous le démontre, cette variable présente un problème puisque toutes les lignes ont au moins un sinistre

Comme discuté dans le hackathon **on ignore cette variable**

```{r echo=FALSE}
expo_tot <- sum(expo_train$EXPO)


dt <-expo_train %>%
  group_by(NBSIN_TYPE1_AN2) %>%
  summarise(nb_lignes = n(), EXPO=sum(EXPO) ,  pct_EXPO = sum(EXPO) /  expo_tot )


formattable(dt,
            align=c('l','r','r', 'r'),
            list(
                nb_lignes= accounting,
                EXPO = accounting,
                pct_EXPO = percent
            )
)


dt %>%
  ggplot( aes(x=as.factor(NBSIN_TYPE1_AN2), y=EXPO, fill=as.factor(NBSIN_TYPE1_AN2))) +
    geom_bar(stat='identity') +
    ggtitle('Exposition par NBSIN_TYPE1_AN2')+
    xlab('NBSIN_TYPE1_AN2') +
    ylab('exposition') +
    labs(fill = 'NBSIN_TYPE1_AN2')
  
```

### **`NBSIN_TYPE1_AN3`** :

variable numérique indiquant le nombre de sinistre de type 1 il y a 3 ans

```{r echo=FALSE}
expo_tot <- sum(expo_train$EXPO)


dt <-expo_train %>%
  group_by(NBSIN_TYPE1_AN3) %>%
  summarise(nb_lignes = n(), EXPO=sum(EXPO) ,  pct_EXPO = sum(EXPO) /  expo_tot )


formattable(dt,
            align=c('l','r','r', 'r'),
            list(
                nb_lignes= accounting,
                EXPO = accounting,
                pct_EXPO = percent
            )
)


dt %>%
  ggplot( aes(x=as.factor(NBSIN_TYPE1_AN3), y=EXPO, fill=as.factor(NBSIN_TYPE1_AN3))) +
    geom_bar(stat='identity') +
    ggtitle('Exposition par NBSIN_TYPE1_AN3')+
    xlab('NBSIN_TYPE1_AN3') +
    ylab('exposition') +
    labs(fill = 'NBSIN_TYPE1_AN3')
  
```

### **`NBSIN_TYPE2_AN1`** :

variable numérique indiquant le nombre de sinistre de type 2 l'année précédente

```{r echo=FALSE}
expo_tot <- sum(expo_train$EXPO)


dt <-expo_train %>%
  group_by(NBSIN_TYPE2_AN1) %>%
  summarise(nb_lignes = n(), EXPO=sum(EXPO) ,  pct_EXPO = sum(EXPO) /  expo_tot )


formattable(dt,
            align=c('l','r','r', 'r'),
            list(
                nb_lignes= accounting,
                EXPO = accounting,
                pct_EXPO = percent
            )
)


dt %>%
  ggplot( aes(x=as.factor(NBSIN_TYPE2_AN1), y=EXPO, fill=as.factor(NBSIN_TYPE2_AN1))) +
    geom_bar(stat='identity') +
    ggtitle('Exposition par NBSIN_TYPE2_AN1')+
    xlab('NBSIN_TYPE2_AN1') +
    ylab('exposition') +
    labs(fill = 'NBSIN_TYPE2_AN1')
  
```

### **`NBSIN_TYPE2_AN2`** :

variable numérique indiquant le nombre de sinistre de type 2 il y a 2 ans

```{r echo=FALSE}
expo_tot <- sum(expo_train$EXPO)


dt <-expo_train %>%
  group_by(NBSIN_TYPE2_AN2) %>%
  summarise(nb_lignes = n(), EXPO=sum(EXPO) ,  pct_EXPO = sum(EXPO) /  expo_tot )


formattable(dt,
            align=c('l','r','r', 'r'),
            list(
                nb_lignes= accounting,
                EXPO = accounting,
                pct_EXPO = percent
            )
)


dt %>%
  ggplot( aes(x=as.factor(NBSIN_TYPE2_AN2), y=EXPO, fill=as.factor(NBSIN_TYPE2_AN2))) +
    geom_bar(stat='identity') +
    ggtitle('Exposition par NBSIN_TYPE2_AN2')+
    xlab('NBSIN_TYPE2_AN2') +
    ylab('exposition') +
    labs(fill = 'NBSIN_TYPE2_AN2')
  
```

### **`NBSIN_TYPE2_AN3`** :

variable numérique indiquant le nombre de sinistre de type 2 il y a 3 ans

```{r echo=FALSE}
expo_tot <- sum(expo_train$EXPO)


dt <-expo_train %>%
  group_by(NBSIN_TYPE2_AN3) %>%
  summarise(nb_lignes = n(), EXPO=sum(EXPO) ,  pct_EXPO = sum(EXPO) /  expo_tot )


formattable(dt,
            align=c('l','r','r', 'r'),
            list(
                nb_lignes= accounting,
                EXPO = accounting,
                pct_EXPO = percent
            )
)


dt %>%
  ggplot( aes(x=as.factor(NBSIN_TYPE2_AN3), y=EXPO, fill=as.factor(NBSIN_TYPE2_AN3))) +
    geom_bar(stat='identity') +
    ggtitle('Exposition par NBSIN_TYPE2_AN3')+
    xlab('NBSIN_TYPE2_AN3') +
    ylab('exposition') +
    labs(fill = 'NBSIN_TYPE2_AN3')
  
```

### **`id`** :

identifiant du risque, clef de jointure

### **`ANNEE`** :

variable catégorielle indiquant l'année d'observation du risque : `2016`, `2017`, `2018`

```{r echo=FALSE}
expo_tot <- sum(expo_train$EXPO)


dt <-expo_train %>%
  group_by(ANNEE) %>%
  summarise(nb_lignes = n(), EXPO=sum(EXPO) ,  pct_EXPO = sum(EXPO) /  expo_tot )


formattable(dt,
            align=c('l','r','r', 'r'),
            list(
                nb_lignes= accounting,
                EXPO = accounting,
                pct_EXPO = percent
            )
)


dt %>%
  ggplot( aes(x=as.factor(ANNEE), y=EXPO, fill=as.factor(ANNEE))) +
    geom_bar(stat='identity') +
    ggtitle('Exposition par ANNEE')+
    xlab('ANNEE') +
    ylab('exposition') +
    labs(fill = 'ANNEE')
  
```

## sin_train

Le fichier `expo_train.csv` contient la liste des contrats en risques historiquement suivis :

```{r}
sin_train = read.table(file = here('data', 'raw', 'sin_train.csv'), header=T, sep=';', dec=',', encoding = 'UTF-8', stringsAsFactors = F)

datatable(head(sin_train))

str(sin_train)
```

-   **`id`** : identifiant du risque, clef de jointure avec le fichier `expo_train`

-   **`NB`** : Nombre de sinistres de la typologie modélisée (inconnue) sur la période d'observation

```{r echo=FALSE}

dt <-sin_train %>%
  group_by(NB) %>%
  summarise(nb_lignes = n())


formattable(dt,
            align=c('l','r'),
            list(
                nb_lignes= accounting
            )
)


dt %>%
  ggplot( aes(x=as.factor(NB), y=nb_lignes, fill=as.factor(NB))) +
    geom_bar(stat='identity') +
    ggtitle('Distribution NB')+
    xlab('NB') +
    ylab('nombre de lignes') +
    labs(fill = 'NB')
  
```

## fichier combiné

On peut désormais combine les information d'exposition et de sinistres :

```{r}
mrh <- expo_train %>%
      left_join(sin_train, by =c('id','ANNEE')) %>%
      replace_na(list('NB'=0, 'COUT'=0))
```

N'ayant pas accèsau fichier de test utilisé lors du KAckathon, on dissocie ici le fichier mrh en un fichier `train` (70%), un fichier `val` (20%) et un fichier `test` (10%)

```{r}
set.seed(42)

trainIndex<- createDataPartition(mrh$NB>=0, p=.7, list=FALSE, time=1)

mrh_train<-mrh[trainIndex,]
mrh_test<-mrh[ -trainIndex,]

valIndex<- createDataPartition(mrh_test$NB>=0, p=.66, list=FALSE, time=1)
mrh_val<-mrh_test[ valIndex,]
mrh_test<-mrh_test[ -valIndex,]

```

### Taux de sinistres moyen :

```{r}
tx <- sum(mrh_train$NB)/sum(mrh_train$EXPO)

print(paste0('taux de sinistre moyen annuel : ', round(tx * 100,2), '%'))

```

### Taux de sinistre par variable :

#### FORMULE

```{r echo=FALSE, message=FALSE}

df = mrh_train
feature = 'FORMULE'

res<-resume(df, feature)

res$table
res$fig
res$figyr

```

**ENSEIGNEMENT** : Le comportement observé est stable dans le temps. Les niveaux `CONFORT` et `ESSENTIEL` semblent avoir un niveau de risque de fréquence équivalent

#### TYPE_RESIDENCE

```{r echo=FALSE, message=FALSE}

df = mrh_train
feature = 'TYPE_RESIDENCE'

res<-resume(df, feature)

res$table
res$fig
res$figyr
```

**ENSEIGNEMENT** : Le comportement observé est stable dans le temps. Les résidences `SECONDAIRE` ont systématiquement une fréquence moindre

#### TYPE_HABITATION

```{r echo=FALSE, message=FALSE}

df = mrh_train
feature = 'TYPE_HABITATION'

res<-resume(df, feature)

res$table
res$fig
res$figyr
```

**ENSEIGNEMENT** : Le comportement observé **N'est PAS** stable dans le temps. Le type d'habitation est à exclure.

#### NB_PIECES

```{r echo=FALSE, message=FALSE}

df = mrh_train
feature = 'NB_PIECES'

res<-resume(df, feature)

res$table
res$fig
res$figyr
```

**ENSEIGNEMENT** : Le `NB_PIECES` présente des valeurs manquantes. La fréquence tend à augmenter avec le nombre de pièces.

#### SITUATION_JURIDIQUE

```{r echo=FALSE, message=FALSE}

df = mrh_train
feature = 'SITUATION_JURIDIQUE'

res<-resume(df, feature)

res$table
res$fig
res$figyr
```

**ENSEIGNEMENT** : Les propriétaires semblent avoir un fréquence moindre que les locataires de manière consistante, mais avec de la variabilité.

#### NIVEAU_JURIDIQUE

```{r echo=FALSE, message=FALSE}

df = mrh_train
feature = 'NIVEAU_JURIDIQUE'

res<-resume(df, feature)

res$table
res$fig
res$figyr
```

**ENSEIGNEMENT** : Le `NIVEAU_JURIDIQUE` **N'est PAS** stable dans le temps. A exclure.

#### VALEUR_DES_BIENS

```{r echo=FALSE, message=FALSE}

df = mrh_train
feature = 'VALEUR_DES_BIENS'

res<-resume(df, feature)

res$table
res$fig
res$figyr
```

**ENSEIGNEMENT** : La fréquence augmente globalement avec la valeur des biens selon une relation non linéaire

#### OBJETS_DE_VALEUR

```{r echo=FALSE, message=FALSE}

df = mrh_train
feature = 'OBJETS_DE_VALEUR'

res<-resume(df, feature)

res$table
res$fig
res$figyr
```

**ENSEIGNEMENT** : Le `NIVEAU_2` présente une fréquence significativement plus élevée.

#### ZONIER

```{r echo=FALSE, message=FALSE}

df = mrh_train
feature = 'ZONIER'

res<-resume(df, feature)

res$table
res$fig
res$figyr
```

**ENSEIGNEMENT** : Le `ZONIER` présente une fréquence avec beaucoup de variabilité, mais il semble y avoir une croissance par région qu'on peut tester :

```{r echo=FALSE, message=FALSE}

df = mrh_train


test <- df %>% mutate(REGION = substr(ZONIER, 1,1))

feature = 'REGION'


res<-resume(test, feature)

res$table
res$fig
res$figyr

```

#### NBSIN_TYPE1_AN1

```{r echo=FALSE, message=FALSE}

df = mrh_train
feature = 'NBSIN_TYPE1_AN1'

res<-resume(df, feature)

res$table
res$fig
res$figyr
```

**ENSEIGNEMENT** : La fréquence augmente globalement avec le nombre de sinistres de type 1 l'année précédente. Toutefois, il semble surtout q'il existe un effet d'une indicatrice a eu sinistre ou non.

#### NBSIN_TYPE1_AN3

```{r echo=FALSE, message=FALSE}

df = mrh_train
feature = 'NBSIN_TYPE1_AN3'

res<-resume(df, feature)

res$table
res$fig
res$figyr
```

**ENSEIGNEMENT** : La fréquence augmente globalement avec le nombre de sinistres de type 1 il y a 3 ans. Toutefois, il semble surtout q'il existe un effet d'une indicatrice a eu sinistre ou non.

#### NBSIN_TYPE2_AN1

```{r echo=FALSE, message=FALSE}

df = mrh_train
feature = 'NBSIN_TYPE2_AN1'

res<-resume(df, feature)

res$table
res$fig
res$figyr
```

**ENSEIGNEMENT** : Il n'existe pas de lien stable sur la fréquence. A ne pas retenir.

#### NBSIN_TYPE2_AN2

```{r echo=FALSE, message=FALSE}

df = mrh_train
feature = 'NBSIN_TYPE2_AN2'

res<-resume(df, feature)

res$table
res$fig
res$figyr
```

**ENSEIGNEMENT** : Il n'existe pas de lien stable sur la fréquence. A ne pas retenir.

#### NBSIN_TYPE2_AN3

```{r echo=FALSE, message=FALSE}

df = mrh_train
feature = 'NBSIN_TYPE2_AN3'

res<-resume(df, feature)

res$table
res$fig
res$figyr
```

**ENSEIGNEMENT** : Il n'existe pas de lien stable sur la fréquence. A ne pas retenir.

#### ANNEE

```{r echo=FALSE, message=FALSE}

df = mrh_train
feature = 'ANNEE'

res<-resume(df, feature)

res$table
res$fig
```

**ENSEIGNEMENT** : L'année 2016 semble atypique. Le niveau actuel semble plus proche de ceux de 2017 et 2018.

### Nettoyage des données

```{r}
preprocess<- function(dt) {
            res <- dt %>%
            mutate( 
              NB_PIECES = ifelse(is.na(NB_PIECES), 2, NB_PIECES), #2 est le mode de NB_PIECES
              NBSIN_TYPE1_AN1 = ifelse(NBSIN_TYPE1_AN1==0,0,1), #On regroupe les modalités non stables dans le temps
              NBSIN_TYPE1_AN3 = ifelse(NBSIN_TYPE1_AN3==0,0,1), #On regroupe les modalités non stables dans le temps
              REGION =  gsub('[0-9]', '', ZONIER) #création de région
            ) %>%
            select(-c('X', 'TYPE_HABITATION', 'NIVEAU_JURIDIQUE', 'NBSIN_TYPE1_AN2', 'NBSIN_TYPE2_AN1', 'NBSIN_TYPE2_AN2', 'NBSIN_TYPE2_AN3'  )) %>% # On supprime les variables sans lien stable dans le temps
            relocate('id', 'EXPO', 'FORMULE', 'TYPE_RESIDENCE', 'SITUATION_JURIDIQUE', 'OBJETS_DE_VALEUR', 'VALEUR_DES_BIENS', 'NB_PIECES', 'ZONIER', 'REGION', 'NBSIN_TYPE1_AN1', 'NBSIN_TYPE1_AN3', 'ANNEE', 'NB', 'COUT' )

            return(res)
}

df_train <- preprocess(mrh_train)
df_val <- preprocess(mrh_val)
df_test <- preprocess(mrh_test)
```

### Calcul du V de Cramer

On mesude l'association des variables entre elles

```{r}
X_train <- df_train %>% select(-c('id', 'EXPO', 'NB', 'COUT'))


# Initialize empty matrix to store coefficients
empty_m <- matrix(ncol = length(X_train),
            nrow = length(X_train),
            dimnames = list(names(X_train), 
                            names(X_train)))
# Function that accepts matrix for coefficients and data and returns a correlation matrix
calculate_cramer <- function(m, df) {
 for (r in seq(nrow(m))){
   for (c in seq(ncol(m))){
     m[[r, c]] <- lsr::cramersV(X_train[[r]], X_train[[c]])
   }
 }
    return(m)
}

cor_matrix <- calculate_cramer(empty_m ,X_train)

corrplot(cor_matrix)

```

# Modélisation de la fréquence

Dans cette partie, on essaye de modéliser la fréquence des sinistres.

On commence par une modélisation de référence qui consistera en un modèle GLM Poisson classique en tirant partie des enseignements de la partie précédente.

```{r}

h2o.init(nthreads = -1)


```

```{r}
reg_bst_100 = h2o.gbm(y = 'NB', x = names(df_train),
                       distribution = "poisson",
                       offset_column = "EXPO",
                       training_frame = as.h2o(df_train),
                       validation_frame = as.h2o(df_val),
                       ntrees = 100,
                       nfolds = 5,
                       seed = 1)

```

```{r}
plot(reg_bst_100)
```

```{r}
reg0 = glm(NB~1+offset(log(EXPO)),family=poisson,data=df_train)

summary(reg0)

plot(reg0)
```

```{r}

fit_glm <- glm(NB ~ FORMULE + TYPE_RESIDENCE + SITUATION_JURIDIQUE + OBJETS_DE_VALEUR + as.factor(VALEUR_DES_BIENS) + as.factor(NB_PIECES) + REGION + as.factor(NBSIN_TYPE1_AN1) + as.factor(NBSIN_TYPE1_AN3) + as.factor(ANNEE), 
              data=df_train, offset=log(EXPO), family=quasipoisson())

summary(fit_glm)

plot(fit_glm)

```

```{r}
x <- c('FORMULE', 'TYPE_RESIDENCE', 'SITUATION_JURIDIQUE','OBJETS_DE_VALEUR', 'VALEUR_DES_BIENS', 'NB_PIECES', 'REGION', 'NBSIN_TYPE1_AN1', 'NBSIN_TYPE1_AN3', 'ANNEE')
y <- 'NB'
w <- 'EXPO'
```

```{r}
# Input maker
prep_xgb <- function(dat, x) {
  data.matrix(dat[, x, drop = FALSE])
}
# Data interface to XGBoost
dtrain <- xgb.DMatrix(
  prep_xgb(df_train, x), 
  label = df_train[[y]], 
  weight = df_train[[w]]
)
# Parameters chosen by 5-fold grouped CV
params_freq <- list(
  learning_rate = 0.2,
  max_depth = 5,
  alpha = 3,
  lambda = 0.5,
  max_delta_step = 2,
  min_split_loss = 0,
  colsample_bytree = 1,
  subsample = 0.9
)
# Fit
set.seed(1)
fit_xgb <- xgb.train(
  params_freq, 
  data = dtrain,
  nrounds = 580,
  objective = "count:poisson",
  watchlist = list(train = dtrain),
  print_every_n = 100
)

# Save and load model
xgb.save(fit_xgb, "xgb.model")
fit_xgb <- xgb.load("xgb.model")

```

# Model Explanations

The models are ready, so let's shed light into them.

## Setting up explainers

We start by setting up the explainers. These are basically objects that know how to create predictions.

**Crucial: the prediction function needs to work for subsets of datasets, not just for the original model data set.**

```{r}
fl_glm <- flashlight(
  model = fit_glm, label = "GLM", 
  predict_function = function(fit, X) predict(fit, X, type = "response")
)

fl_xgb <- flashlight(
  model = fit_xgb, label = "XGBoost", 
  predict_function = function(fit, X) predict(fit, prep_xgb(X, x))
)

fl_xgb <- flashlight(
  model = fit_xgb, label = "H20 GBM", 
  predict_function = function(fit, X) predict(fit, prep_xgb(X, x))
)

reg_bst_100



# Combine them and add common elements like reference data
metrics <- list(`Average deviance` = deviance_poisson, 
                `Relative deviance reduction` = r_squared_poisson)
fls <- multiflashlight(list(fl_glm, fl_xgb), data = df_val, 
                       y = y, w = w, metrics = metrics)
# Version on canonical scale
fls_log <- multiflashlight(fls, linkinv = log)
```

## Performance

We start to interpret the models by looking at model performance using deviance related metrics.

```{r}
fillc <- "#E69F00"
perf <- light_performance(fls)
perf
plot(perf, geom = "point") +
  labs(x = element_blank(), y = element_blank())
```

## Importance

Next, we consider permutation variable importance. By default `flashlight` uses the first performance metric specified above.

```{r}
imp <- light_importance(fls, v = x)
plot(imp, fill = fillc, color = "black")
```

### Partial dependence curves

Taking the average of many ICE curves produces the famous partial dependence plot. We consider such plots for four predictors.

```{r}
plot(light_profile(fls, v = "VALEUR_DES_BIENS"))
plot(light_profile(fls, v = "NB_PIECES"))
plot(light_profile(fls, v = "TYPE_RESIDENCE"))

```

## Classic diagnostic plots

Classic predicted/residual/response versus covariable plots are worth a look.

```{r}
# Average predicted versus covariable
plot(light_profile(fls, v = "NB_PIECES", type = "predicted"))
# Average residual versus covariable
plot(light_profile(fls, v = "NB_PIECES", type = "residual")) +
  geom_hline(yintercept = 0)
# Average response versus covariable
plot(light_profile(fls, v = "NB_PIECES", type = "response"))
```

### Multiple aspects combined

We often get a good picture of the effect of a covariable by combining partial dependence with classic plots.

```{r}
eff_DrivAge <- light_effects(fls, v = "NB_PIECES", counts_weighted = TRUE)
p <- plot(eff_DrivAge, show_points = FALSE)
plot_counts(p, eff_DrivAge, alpha = 0.3)
```

```{r}
interact_rel <- light_interaction(
  fls_log, 
  v = most_important(imp, 4), 
  take_sqrt = FALSE,
  pairwise = TRUE, 
  use_linkinv = TRUE,
  seed = 61
)
plot(interact_rel, color = "black", fill = fillc, rotate_x = TRUE)
```

### On absolute scale

```{r}
interact_abs <- light_interaction(
  fls_log, 
  v = most_important(imp, 4), 
  normalize = FALSE,
  pairwise = TRUE, 
  use_linkinv = TRUE,
  seed = 61
)
plot(interact_abs, color = "black", fill = fillc, rotate_x = TRUE)
```

### Visualization

Let's illustrate a strong and a weak interaction by conditional partial dependence plots.

```{r}
# Strong interaction
# p <- plot(eff_DrivAge, show_points = FALSE)
# plot_counts(p, eff_DrivAge, alpha = 0.3)


pdp_NBPIECES_REGION <- light_profile(fls_log, v = "NB_PIECES", by = "REGION", 
                                  pd_seed = 50, data = df_val, counts_weighted=TRUE)
p <- plot(pdp_NBPIECES_REGION, show_points = FALSE)
pdp_count <- light_effects(fls, v = "NB_PIECES")
plot_counts(p, pdp_count, alpha = 0.3)

pdp_VALEURDESBIENS_REGION <- light_profile(fls_log, v = "VALEUR_DES_BIENS", by = "REGION", 
                                  pd_seed = 50, data = df_val, counts_weighted=TRUE)
plot(pdp_VALEURDESBIENS_REGION)

# Weak interaction
pdp_TYPE_RESIDENCE_REGION <- light_profile(fls_log, v = "TYPE_RESIDENCE", 
                                 by = "REGION", pd_seed = 50, data = df_val, counts_weighted=TRUE)
plot(pdp_TYPE_RESIDENCE_REGION)
```

```{r}
summary(df_val$NB/df_val$EXPO)
summary(fl_glm$predict_function(fit_glm, df_val))
summary(fl_xgb$predict_function(fit_xgb, df_val))
```

fls
